# GPUを用いた偏微分方程式の差分法数値シミュレーション

## 概要

- 差分法で偏微分方程式の数値シミュレーションをします。
- 差分法の拡散方程式の漸化式は、以下のようになります。
- u\[i\]\[j\] = (1 - 4 * r) * u\[i\]\[j\] + r * (u\[i + 1\]\[j\] + u\[i - 1\]\[j\] + u\[i\]\[j + 1\] + u\[i\]\[j - 1\]);


### CPU版

- 複数スレッドへの対応には openmp を使用しました。
- 学科アカウントの環境では、物理CPUが2個、1物理CPUあたり物理コアが10個、1物理コアあたり4スレッド、論理プロセッサは合計で40個立てられます。

### GPU版(CUDAを使用)

- 1 blockあたりのthread数は1024個を超えてはならないようなので、スレッドのサイズを(32 * 32)までに制限しましたが、果たしてこういう意味なのかどうかはよくわからないです。どうやらblock数も上限があるようで、1 threadあたり32 threadのもとで、32 blockを超えるとおかしな挙動をするようです...

## 参考ページ

- [二次元配列をCUDAで使う方法](https://stackoverflow.com/questions/35771430/cudamallocpitch-and-cudamemcpy2d)
- [エスケープシーケンスによる出力の着色](https://www.mm2d.net/main/prog/c/console-03.html)

## 自分が試した時の結果

- サイズは512 * 512, イテレーションは100回しました。
- 時間計測は、
    - CPUのみだと、約 0.16 秒程度の値が出ました。
    - OpenMPを使うと、約 0.02 ~ 0.04 秒程度の値が出ました。
    - GPUを使うと、約 0.002 秒程度の値が出ました。

## 備考

拡散方程式以外のコードはいずれ整理して消します。
