# GPUを用いた偏微分方程式の差分法数値シミュレーション

## 概要

- 差分法で偏微分方程式の数値シミュレーションをします。
- 差分法の拡散方程式の漸化式は、以下のようになります。
- u\[i\]\[j\] = (1 - 4 * r) * u\[i\]\[j\] + r * (u\[i + 1\]\[j\] + u\[i - 1\]\[j\] + u\[i\]\[j + 1\] + u\[i\]\[j - 1\]);


### CPU版

- openmp を使用しました。
- 学科アカウントの環境では、物理CPUが2個、1物理CPUあたり物理コアが10個、1物理コアあたり4スレッド、論理プロセッサは合計で40個立てられます。

### GPU版(CUDAを使用)

- 1 blockあたりのthread数は1024個を超えてはならないようなので、スレッドのサイズを(32 * 32)までに制限しましたが、果たしてこういう意味なのかどうかはよくわからないです。どうやらblock数も上限があるようで、1 threadあたり32 threadのもとで、32 blockを超えるとおかしな挙動をするようです...

## 参考ページ

## 自分が試した時の結果

## 備考
拡散方程式以外のコードはいずれ整理して消します。
